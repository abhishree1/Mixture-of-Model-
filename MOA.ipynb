{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wr6b7Vc5KqV2",
        "outputId": "2f9220ad-83d7-482e-b952-8b8fc5c4a8f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)',\n",
              " 'Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)',\n",
              " 'Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)',\n",
              " 'Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)',\n",
              " 'Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)',\n",
              " 'Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)',\n",
              " 'Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)',\n",
              " 'Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)',\n",
              " 'Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)',\n",
              " 'Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)',\n",
              " 'Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)',\n",
              " 'Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)',\n",
              " 'Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)',\n",
              " 'Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)',\n",
              " 'Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)',\n",
              " 'Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)',\n",
              " 'Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!!pip install transformers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpaXb-d9LQ11"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4OOf1BTJ2mh"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "from typing import List\n",
        "\n",
        "class LLMClient:\n",
        "    def __init__(self, model_name: str):\n",
        "        self.model_name = model_name\n",
        "        print(f\"Loading model: {model_name}\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
        "        self.pipeline = pipeline(\"text-generation\", model=self.model, tokenizer=self.tokenizer, device=0)\n",
        "\n",
        "    def generate(self, prompt: str, max_tokens: int = 256) -> str:\n",
        "        response = self.pipeline(prompt, max_new_tokens=max_tokens, temperature=0.7, top_k=50, do_sample=True)\n",
        "        return response[0]['generated_text']\n",
        "\n",
        "\n",
        "class MOA:\n",
        "    def __init__(self, proposer_clients: List[LLMClient], num_layers: int):\n",
        "        self.proposer_clients = proposer_clients\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def generate_responses(self, prompt: str) -> List[str]:\n",
        "        responses = []\n",
        "        for client in self.proposer_clients:\n",
        "            try:\n",
        "                response = client.generate(prompt)\n",
        "                responses.append(response)\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating response from {client.model_name}: {e}\")\n",
        "        return responses\n",
        "\n",
        "    def aggregate_responses(self, responses: List[str], aggregator: LLMClient) -> str:\n",
        "        prompt = \"You are tasked to synthesize these responses into a single, high-quality answer:\\n\\n\"\n",
        "        for i, response in enumerate(responses):\n",
        "            prompt += f\"Response {i + 1}: {response}\\n\\n\"\n",
        "        prompt += \"Provide a refined and comprehensive response based on the above.\"\n",
        "        return aggregator.generate(prompt, max_tokens=512)\n",
        "\n",
        "    def run(self, initial_prompt: str, aggregator: LLMClient) -> str:\n",
        "        current_prompt = initial_prompt\n",
        "        for layer in range(self.num_layers):\n",
        "            print(f\"Processing Layer {layer + 1}/{self.num_layers}...\")\n",
        "\n",
        "            proposer_responses = self.generate_responses(current_prompt)\n",
        "\n",
        "            current_prompt = self.aggregate_responses(proposer_responses, aggregator)\n",
        "\n",
        "        return current_prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZVwyWLvMMBB"
      },
      "outputs": [],
      "source": [
        "proposer_models = [\n",
        "        LLMClient(\"Qwen/Qwen1.5-110B-Chat\"),\n",
        "        LLMClient(\"Qwen/Qwen1.5-72B-Chat\"),\n",
        "        LLMClient(\"alpindale/WizardLM-2-8x22B\"),\n",
        "        LLMClient(\"mistralai/Mixtral-8x22B-v0.1\"),\n",
        "        LLMClient(\"databricks/dbrx-instruct\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flqxND4cMPPT"
      },
      "outputs": [],
      "source": [
        "aggregator = LLMClient(\"Qwen/Qwen1.5-110B-Chat\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ly07k7cjMRYu"
      },
      "outputs": [],
      "source": [
        "moa = MOA(proposer_clients=proposer_models, num_layers=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wS_krZR0NWXP"
      },
      "outputs": [],
      "source": [
        "initial_prompt = \"What are the advantages and disadvantages of array?\"\n",
        "response = moa.run(initial_prompt, aggregator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFWo5iN9m1WT"
      },
      "outputs": [],
      "source": [
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
